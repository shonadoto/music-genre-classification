# Классификация музыкальных жанров по аудио

## Илья Ванков

## Постановка задачи

В проекте решается задача автоматической классификации музыкального жанра по аудиозаписи.
Система получает на вход короткий аудиофрагмент длительностью 30 секунд и должна определить один из 10 жанров: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock.

Такая задача востребована в музыкальных сервисах, медиаплатформах и библиотеках, где требуется автоматически разметить большие объёмы аудиофайлов, улучшить рекомендации или структурировать коллекцию без ручной разметки.

## Формат входных и выходных данных

### Входные данные

- Набор данных состоит из 1000 звуковых дорожек по 30 секунд каждая. Он содержит 10 жанров, каждый из которых представлен 100 треками. Все треки представляют собой 16-битные монофонические аудиофайлы с частотой 22050 Гц в формате .wav
- Mel-спектрограмма этих аудиофайлов
- 2 файла CSV — содержат характеристики аудиофайлов. В одном файле для каждой песни (длиной 30 секунд) указано среднее значение и дисперсия, рассчитанные на основе нескольких характеристик, которые можно извлечь из аудиофайла. Другой файл имеет ту же структуру, но песни были предварительно разделены на аудиофайлы длительностью 3 секунды

### Выходные данные

- вероятностное распределение по 10 жанрам
- финальное решение — жанр с максимальной вероятностью

## Метрики

Основные метрики:

1. **Accuracy** - базовая метрика для многоклассовой классификации.
   На GTZAN типичные значения для CNN-моделей — **0.70–0.85**
2. **Macro F1-score** - позволяет корректно учитывать возможный дисбаланс и разные сложности жанров
3. **Confusion matrix** - нужна скорее для анализа, т.к. некоторые жанры похожи (metal/rock, jazz/blues)

## Валидация и тест

Датасет будет разделён на три части:

- train / validation / test, 70% / 15% / 15%
- стратифицированное разбиение по жанрам

Чтобы избежать утечки информации и сделать процесс воспроизводимым:

- используется фиксированный random_seed
- разделять выборки по аудиофайлам, чтобы трейн не просочился в валидацию и тест

## Датасеты

**GTZAN Genre Dataset**

### Содержимое:

1. **Аудио (genres_original)**
   - 1000 WAV-файлов
   - 10 жанров по 100 треков
   - длительность каждого — 30 секунд
   - формат: WAV, 22050 Hz, mono
2. **Изображения (images_original)**
   - mel-спектрограммы, предобработанные авторами
   - подходят для CNN-моделей
3. **CSV-файлы с признаками**
   - версия со сводными признаками
   - версия с нарезкой на 3-секундные сегменты (объём данных увеличен в 10 раз)

## Моделирование

### Бейзлайн

В качестве baseline будет реализован простой классификатор:

- использовать готовую mel-спектрограмму
- усреднить её по времени
- подать в двухслойную полносвязную сеть или логистическую регрессию

### Основная модель

Основная модель будет нейросетевой:

**CNN-классификатор поверх mel-спектрограмм**

План:

1. Преобразовать аудио в mel-спектрограмму (librosa)
2. Нормализовать её и рассматривать как одноканальное изображение
3. Применить сверточную архитектуру, простая 4–6-слойная CNN

Используемые библиотеки:

- PyTorch
- torchaudio
- librosa для аудио-признаков

Параметры обучения:

- оптимизатор Adam
- learning rate ~ 1e–4
- батчи 16–64
- ранняя остановка по валидации

## Setup

1. **Prerequisites:**
   - Python 3.13.x
   - Poetry для управления зависимостями
   - Git
   - Pre-commit (опционально, будет также установлен через Poetry)
   - Cuda — для работы с GPU (опционально, если планируется использовать GPU)

2. **Клонирование репозитория:**

   ```bash
   git clone <repository_url>
   cd music-genre-classification
   ```

3. **Установка зависимостей:**

   ```bash
   poetry env use python3.13
   poetry lock
   poetry install
   poetry run pre-commit install
   ```

4. **Загрузка данных:**
   Данные хранятся через DVC. При первом запуске `train` или `infer` данные подтягиваются через DVC.
   Если данных нет в DVC-хранилище, автоматически вызывается `download_data()`, затем аудио преобразуется в mel-спектрограммы, и уже они сохраняются в DVC (`data/dataset/spectrograms`).
   Ссылка зашита константой в `music_genre_classification/data/data_manager.py`:

5. **Проверка хуков:**
   ```bash
   poetry run pre-commit run -a
   ```

## Train

Все команды запускаются из корневой директории проекта.

1. **Запуск сервера MLflow:**
   Если сервер уже запущен, то можно пропустить этот шаг. Если нет, то запустите его:

   ```bash
   poetry run mlflow server --host 127.0.0.1 --port 8080 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns
   ```

2. **Настройка конфигов (опционально, при желании что-то изменить):**
   Конфигурирование проекта осуществляется через Hydra. Конфигурационные файлы находятся в директории `configs/`. Основной конфиг для обучения находится в `configs/train.yaml`. Вы можете изменить параметры обучения, такие как количество эпох, акселератор (GPU/CPU), папку для сохранения графиков, и другие параметры.
   В поддиректории `configs/datamodule/` находятся конфиги для настройки датасета, в поддиректории `configs/model/` — для настройки модели, а в `configs/logger/` — для настройки логирования.

3. **Запуск обучения:**
   Для запуска обучения используйте следующую команду:
   ```bash
   poetry run python -m music_genre_classification.commands train
   ```
   Во время первого запуска данные будут получены через DVC (`pull`). Если в хранилище данных ещё нет, проект скачает архив с Google Drive, построит спектрограммы и добавит их в DVC.
   Во время обучения вы можете следить за процессом в MLFlow UI, расположенном по адресу, указанному в конфиге (по умолчанию http://127.0.0.1:8080). Сохраняются следующие метрики: loss (train и val), accuracy (train и val), f1-score (train и val), epoch, а также графики обучения (вкладка Model metrics).
   После завершения обучения, информация о запуске будет сохранена в папку `outputs/{date}/{time}/`. Там будут находиться:
   - Конфигурационный файл с параметрами обучения.
   - Сама обученная модель в формате `*.ckpt`.
     Также, в папке `plots/{run_name}` (где `run_name` — это имя запуска из MLFlow) будут сохранены графики обучения.

## Production preparation

После обучения используются следующие артефакты:

- `outputs/model_full.ckpt` — ссылка на лучший чекпоинт по `val/loss`;
- `outputs/checkpoints/*.ckpt` — сохранённые чекпоинты;
- `data/dataset/spectrograms` — предобработанные данные (под управлением DVC).

Подготовка к запуску в проде в рамках этого проекта:

1. Выбрать модель для деплоя (`outputs/model_full.ckpt`).
2. Проверить её на тесте:
   ```bash
   poetry run python -m music_genre_classification.commands test outputs/model_full.ckpt
   ```
3. Запустить deployment (Telegram-бот) с этой моделью:
   ```bash
   poetry run python -m music_genre_classification.commands bot outputs/model_full.ckpt --token <telegram_token>
   ```

## Infer

Для запуска инференса на одном аудио файле используйте следующую команду:

```bash
poetry run python -m music_genre_classification.commands infer --model_path <path_to_model> --audio_path <path_to_audio> [--output_path <path_to_output>]
```

Например (если вы хотите сделать инференс на последней модели):

```bash
poetry run python -m music_genre_classification.commands infer --model_path outputs/model_full.ckpt --audio_path data/dataset/genres_original/blues/blues.00000.wav --output_path result.txt
```

Формат входных данных: WAV файлы с частотой дискретизации 22050 Hz, моно, длительностью 30 секунд.

Выходные данные: предсказанный жанр и вероятности по всем классам.

## Test

Запуск тестирования обученной модели:

```bash
poetry run python -m music_genre_classification.commands test outputs/model_full.ckpt
```

Команда использует те же конфиги датамодуля и тренера, что и обучение, и выводит test-метрики в консоль.

## Deployment (Telegram bot)

Минимальная часть внедрения реализована через Telegram-бота в одном приложении с моделью.

1. Создайте бота через `@BotFather` и получите токен.
2. Запустите бота:

```bash
poetry run python -m music_genre_classification.commands bot outputs/model_full.ckpt --token <telegram_token>
```

или через переменную окружения:

```bash
export TELEGRAM_BOT_TOKEN=<telegram_token>
poetry run python -m music_genre_classification.commands bot outputs/model_full.ckpt
```

Бот принимает аудио в популярных форматах (`.wav`, `.mp3`, `.ogg`, `.m4a`, `.flac`),
при необходимости конвертирует во временный `.wav`, запускает инференс и возвращает предсказанный жанр с вероятностями.
Если файл длиннее 30 секунд, бот автоматически режет его на чанки по 30 секунд и усредняет вероятности по всем чанкам.
